# Neural Network Academy

**School Project:** MaskininlÃ¤rning - Projekt II (Variant 2)

An educational neural network platform with **32 progressive learning problems** (from basic logic gates to CNNs), implemented from scratch using only NumPy. Features **guided learning paths**, interactive web-based visualization, adaptive training, real-time progress tracking, and embedded systems deployment for Raspberry Pi.

---

## Quick Start

```bash
# Terminal 1: Start backend
cd backend
pip install -r requirements.txt
python app.py

# Terminal 2: Start frontend
cd frontend
npm install
npm run dev
```

Open http://localhost:5173 in your browser.

---

## ğŸ†• What's New

**Interactive Learning Challenges (Latest):**
- ğŸ® **Build Challenges** - Drag-and-drop network architecture builder
- ğŸ¤” **Prediction Quizzes** - Predict outcomes before training to build intuition
- ğŸ› **Debug Challenges** - Diagnose broken configurations from symptoms
- ğŸ¯ **New "Interactive Fundamentals" Path** - Learn by DOING, not just watching

**Learning Paths Feature:**
- ğŸ“ 7 guided learning paths with step-by-step journeys
- ğŸ“Š Real-time progress tracking with localStorage persistence
- ğŸ’¡ Progressive hint system (unlocks based on attempts)
- ğŸ‰ Celebration modal with animated badges and confetti
- ğŸ”„ Resume from where you left off

**Enhanced Training Experience:**
- ğŸ“¢ Training Narrator - Real-time insights during training
- ğŸ’¥ Failure Dramatization - Visual effects for failure cases
- ğŸ“ˆ Enhanced Loss Curve - Tooltips and annotations
- ğŸ† Achievement System - Earn badges for milestones

---

## âœ¨ Key Features

- ğŸ® **Interactive Challenges** - Build networks, predict outcomes, debug problems
- ğŸ“ **7 Learning Paths** - Guided journeys from beginner to advanced
- ğŸ§  **32 Progressive Problems** - From basic gates to CNNs (7 difficulty levels)
- ğŸ¨ **Interactive Visualization** - Real-time network diagram, decision boundaries, 3D loss landscapes
- ğŸ¯ **Adaptive Training** - Auto-adjusts learning rate to reach ~99% accuracy
- ğŸ’¡ **Failure Case Education** - Learn from intentional failures (bad LR, zero init, vanishing gradients)
- ğŸ“Š **Live Training Insights** - Real-time narrator explains what's happening
- âŒ¨ï¸ **Keyboard Shortcuts** - Space (train), Escape (stop), R (reset), S (step)
- ğŸ† **Achievement System** - Badges, milestones, progress tracking
- ğŸ“± **Responsive Design** - Works on desktop and mobile
- ğŸ”§ **Pure NumPy** - No ML frameworks, educational from first principles
- ğŸ® **Embedded Support** - Runs on Raspberry Pi with GPIO

---

## Learning Paths ğŸ“

The platform now features **guided learning paths** that provide structured, step-by-step journeys through neural network concepts.

### Available Paths

**Interactive Fundamentals** (Beginner) â­ NEW
- 7 interactive steps: build, predict, debug, train
- Learn by DOING - not just watching
- Drag-and-drop architecture builder
- Prediction quizzes and debug challenges
- Badge: ğŸ® "Active Learner"

**Foundations** (Beginner)
- 7 steps covering single neurons and XOR
- Learn linear separability and why hidden layers matter
- Badge: ğŸ† "Foundation Scholar"

**Deep Learning Basics** (Intermediate)
- 10 steps on training, initialization, hyperparameters
- Includes failure case demonstrations
- Badge: ğŸ§  "Neural Navigator"

**Multi-Class Mastery** (Intermediate)
- 4 steps exploring multi-class classification
- Softmax, one-hot encoding, probability outputs
- Badge: ğŸ¨ "Classifier Champion"

**Pitfall Prevention** (Intermediate)
- 6 steps teaching what NOT to do
- Learn from intentional failures
- Badge: ğŸ›¡ï¸ "Error Expert"

**Convolutional Vision** (Advanced)
- 3 steps on CNNs for image data
- Shape detection and digit recognition
- Badge: ğŸ‘ï¸ "Vision Virtuoso"

**Research Frontier** (Advanced)
- 4 steps tackling challenging problems
- Spirals, donuts, complex surfaces
- Badge: ğŸš€ "Research Pioneer"

### Screenshots

![Learning Path Selector](docs/screenshots/path-selector.png)
*Choose from curated learning paths with progress tracking*

![Path Detail View](docs/screenshots/path-detail.png)
*Step-by-step interface with hints and progress visualization*

![Completion Modal](docs/screenshots/completion.png)
*Celebrate your achievement with animated badge and confetti*

### How It Works

1. **Select a Path** - Click "Learning Paths" in the header
2. **Step-by-Step Progress** - Complete problems in sequence
3. **Auto-Unlock** - Next step unlocks when you reach required accuracy
4. **Hint System** - Unlock hints after multiple attempts (1 hint per 2 attempts)
5. **Progress Tracking** - Your progress persists in localStorage
6. **Celebration** - Complete all steps to earn your badge with confetti animation! ğŸ‰

### Features

- âœ… **Progress Persistence** - Pick up where you left off
- âœ… **Visual Progress Bar** - See your journey at a glance
- âœ… **Smart Hints** - Get help when you need it
- âœ… **Completion Detection** - Auto-advances on success
- âœ… **Achievement Badges** - Earn rewards for completion
- âœ… **Failure Case Support** - Some steps teach by failing intentionally

---

## Project Overview

### What it does

This platform offers **32 learning problems** organized into **7 progressive difficulty levels**:

1. **Level 1: Single Neuron** - AND, OR, NOT, NAND gates
2. **Level 2: Hidden Layers Required** - XOR, XNOR, 5-bit parity
3. **Level 3: 2D Decision Boundaries** - Blobs, moons, circles, spirals
4. **Level 4: Regression** - Linear, sine wave, polynomial, 2D surfaces
5. **Level 5: Failure Cases** - Demonstrates common pitfalls (bad LR, zero init, vanishing gradients)
6. **Level 6: Multi-Class** - Quadrant classification, Gaussian blobs, color recognition
7. **Level 7: CNNs** - Shape detection, digit recognition on 8Ã—8 grids

Each problem teaches specific neural network concepts through interactive visualization.

### System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5 Buttons  â”‚ â”€â”€â–¶ â”‚  Neural Network  â”‚ â”€â”€â–¶ â”‚    LED      â”‚
â”‚  (GPIO in)  â”‚     â”‚  [5,12,8,4,1]    â”‚     â”‚  (GPIO out) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Web Dashboard  â”‚
                    â”‚  (Visualization) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Requirements Coverage

### G-Level (Pass)

| # | Requirement | Implementation |
|---|-------------|----------------|
| **G1** | NN from scratch (no ML libs) | `neural_network.py` - Pure NumPy |
| **G2** | Clean code, documented | Type hints, docstrings throughout |
| **G3** | 1 hidden layer, user-configurable | `NeuralNetwork([5, 8, 1])` |
| **G4** | Static training (user params) | `train(epochs=1000, lr=0.5)` |
| **G5** | Train 4-bit XOR | We do 5-bit for VG10 |
| **G6** | Terminal output on change | Prints prediction on button press |
| **G7** | GitHub + live demo | This repo |

### VG-Level (Higher Grade)

| # | Requirement | Implementation |
|---|-------------|----------------|
| **VG8** | Arbitrary hidden layers | `NeuralNetwork([5, 12, 8, 4, 1])` |
| **VG9** | Adaptive training to ~100% | `train_adaptive()` - auto LR adjustment + restarts |
| **VG10** | 5-bit XOR, 3-5 hidden layers | Default architecture has 3 hidden layers |

---

## Neural Network Theory

### Network Structure

```
Input Layer      Hidden Layers           Output Layer
(5 neurons)      (configurable)          (1 neuron)

    â—‹              â—‹    â—‹    â—‹              â—‹
    â—‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶   â—‹    â—‹    â—‹  â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚
    â—‹              â—‹    â—‹    â—‹              LED ON/OFF
    â—‹              â—‹    â—‹    â—‹
    â—‹              â—‹    â—‹
                   â—‹    â—‹

  Buttons        ReLU  ReLU  ReLU         Sigmoid
```

### Key Concepts

1. **Forward Propagation**: Data flows through layers via matrix multiplication
2. **Activation Functions**: ReLU (hidden), Sigmoid (output)
3. **Backpropagation**: Compute gradients using chain rule
4. **Gradient Descent**: Update weights to minimize loss

### Why XOR needs hidden layers

XOR is not linearly separable - you cannot draw a straight line to separate the classes. This is why a single-layer perceptron cannot learn XOR, proving the need for hidden layers.

```
Standard XOR (2 inputs):     Linear attempt (fails):

  0,1 â— â”€ â”€ â”€ â— 1,1            0,1 â—       â— 1,1
      â”‚       â”‚                    â”‚ â•²   â•± â”‚
      â”‚  XOR  â”‚                    â”‚  â•² â•±  â”‚  â† No single line works!
      â”‚       â”‚                    â”‚ â•± â•²   â”‚
  0,0 â— â”€ â”€ â”€ â— 1,0            0,0 â—       â— 1,0
```

---

## Project Structure

```
neural-network-academy/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                 # Flask API + WebSocket server
â”‚   â”œâ”€â”€ neural_network.py      # Pure NumPy dense network
â”‚   â”œâ”€â”€ cnn_network.py         # Pure NumPy CNN implementation
â”‚   â”œâ”€â”€ cnn_layers.py          # Conv2D, MaxPool2D, Flatten layers
â”‚   â”œâ”€â”€ problems.py            # 32 problem definitions (7 levels)
â”‚   â”œâ”€â”€ learning_paths.py      # Learning path definitions
â”‚   â”œâ”€â”€ gpio_simulator.py      # Raspberry Pi GPIO simulation
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ tests/                 # Comprehensive pytest suite
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx            # Main orchestrator
â”‚   â”‚   â”œâ”€â”€ components/        # 28 specialized components
â”‚   â”‚   â”‚   â”œâ”€â”€ ProblemSelector.tsx          # Level/problem navigation
â”‚   â”‚   â”‚   â”œâ”€â”€ LearningPathSelector.tsx     # Path selection grid
â”‚   â”‚   â”‚   â”œâ”€â”€ LearningPathCard.tsx         # Path card with progress
â”‚   â”‚   â”‚   â”œâ”€â”€ PathDetailView.tsx           # Step-by-step path interface
â”‚   â”‚   â”‚   â”œâ”€â”€ PathProgressBar.tsx          # Visual step progress
â”‚   â”‚   â”‚   â”œâ”€â”€ PathStepCard.tsx             # Step info display
â”‚   â”‚   â”‚   â”œâ”€â”€ StepHintPanel.tsx            # Progressive hint reveal
â”‚   â”‚   â”‚   â”œâ”€â”€ PathCompletionModal.tsx      # Celebration screen
â”‚   â”‚   â”‚   â”œâ”€â”€ InputPanel.tsx               # Adaptive input controls
â”‚   â”‚   â”‚   â”œâ”€â”€ NetworkVisualization.tsx     # SVG network diagram
â”‚   â”‚   â”‚   â”œâ”€â”€ CNNEducationalViz.tsx        # CNN feature maps
â”‚   â”‚   â”‚   â”œâ”€â”€ TrainingPanel.tsx            # Training controls
â”‚   â”‚   â”‚   â”œâ”€â”€ LossCurve.tsx                # Recharts integration
â”‚   â”‚   â”‚   â”œâ”€â”€ DecisionBoundaryViz.tsx      # 2D boundary plotting
â”‚   â”‚   â”‚   â”œâ”€â”€ LossLandscape3D.tsx          # 3D loss surface (Three.js)
â”‚   â”‚   â”‚   â””â”€â”€ ... (13 more components)
â”‚   â”‚   â””â”€â”€ hooks/
â”‚   â”‚       â”œâ”€â”€ useSocket.ts                 # WebSocket management
â”‚   â”‚       â”œâ”€â”€ usePathProgress.ts           # Progress tracking
â”‚   â”‚       â””â”€â”€ useKeyboardShortcuts.ts      # Keyboard controls
â”‚   â”œâ”€â”€ tests/                 # Playwright E2E tests (109 tests)
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ README.md
```

---

## API Endpoints

### Core Endpoints
```
GET  /api/status              # System status
GET  /api/network             # Network architecture + weights
POST /api/network/architecture # Change architecture
POST /api/train               # Start static training (G4)
POST /api/train/adaptive      # Start adaptive training (VG9)
POST /api/train/stop          # Stop training
POST /api/train/step          # Single epoch (step-by-step)
POST /api/network/reset       # Reset weights
GET  /api/gpio                # GPIO state
POST /api/gpio/button/<id>    # Toggle button
GET  /api/predict             # Current prediction
```

### Learning Paths API
```
GET  /api/paths               # List all learning paths
GET  /api/paths/<id>          # Get path details with steps
GET  /api/problems            # List all 32 problems
GET  /api/problems/<id>       # Get problem info
POST /api/problems/<id>/select # Switch to problem
```

### WebSocket Events
```
training_started              # Training begins
training_progress             # Epoch update (loss, accuracy)
training_complete             # Training finished
problem_changed               # Problem switched
prediction                    # Real-time prediction
```

---

## Testing

### Backend Tests (Pytest)
```bash
cd backend
pytest                        # Run all tests
pytest -v                     # Verbose output
pytest tests/test_network.py  # Specific test file
```

### Frontend Tests (Playwright E2E)
```bash
cd frontend
npm run test                  # Run E2E tests
npm run test:ui              # Interactive test UI
```

**Test Coverage:**
- âœ… Backend: 358 tests passing
- âœ… Frontend E2E: Interactive challenges (14/14)
- âœ… Frontend E2E: New features (9/9)
- âœ… Frontend E2E: Learning paths verified

---

## Running on Raspberry Pi

To run on actual Raspberry Pi hardware:

1. Install lgpio: `pip install lgpio`
2. Connect buttons to GPIO pins 17, 27, 22, 23, 24
3. Connect LED to GPIO pin 18
4. In `app.py`, replace `GPIOSimulator` with `GPIOHardware`

---

## Evaluation (UtvÃ¤rdering)

### 1. Vad lÃ¤rde ni er av projektet?

- Deep understanding of backpropagation by implementing it manually
- How matrix operations make neural networks efficient
- Why weight initialization (Xavier) matters for training convergence
- The importance of learning rate - too high causes instability, too low is slow
- Why XOR requires hidden layers (not linearly separable)
- How to design educational user experiences with guided learning paths
- Progressive enhancement - building complex features on solid foundations

### 2. Vad var lÃ¤tt/svÃ¥rt?

**Easy:**
- Setting up Flask API and React frontend
- Understanding forward propagation (just matrix multiplication)
- GPIO simulation

**Difficult:**
- Getting backpropagation correct (chain rule application)
- Debugging when the network wouldn't converge to 100%
- Understanding why different weight initializations give different results

### 3. Vad hade ni velat ha lÃ¤rt er mer innan projektet?

- More about calculus and the chain rule
- Different optimization algorithms (Adam, momentum)
- How to visualize what the network is learning

### 4. Ã–vriga kommentarer?

Building from scratch without ML libraries was challenging but educational. The interactive web frontend made it much easier to understand what's happening during training. Adaptive training with automatic restarts was key to reliably reaching 100% accuracy.

The addition of guided learning paths transforms this from a demonstration into a complete educational platform. Progress tracking, hint systems, and celebration animations make learning neural networks engaging and rewarding.

---

## License

MIT
